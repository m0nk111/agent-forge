# Example configuration for Caramba Issue #7 (Personality AI)
# Copy this to a new file and customize for your project

project:
  name: "Caramba Personality AI"
  root: "/home/flip/caramba"
  issue: "Issue #7"

model:
  name: "qwen2.5-coder:7b"
  ollama_url: "http://localhost:11434"

context:
  description: "Caramba AI platform - Personality AI Service with LLM integration and RAG pipeline"
  structure: |
    - app/services/ - Service modules (face_swap, voice-training, etc.)
    - app/backend/api/ - FastAPI REST endpoints
    - app/frontend/ - React/TypeScript frontend
    - docs/ - Documentation
  
  tech_stack:
    - "Backend: FastAPI, Python 3.12, async/await"
    - "LLM: Tars-AI (192.168.1.26:8001) with Ollama fallback (localhost:11434)"
    - "RAG: LangChain or LlamaIndex, sentence-transformers, FAISS or ChromaDB"
    - "Database: PostgreSQL with Alembic migrations"
    - "Testing: pytest with async support"
  
  reference_services:
    - "app/services/face_swap/ - SadTalker and Wav2Lip integration"
    - "app/services/voice-training/ - AudioTransfer with Opus compression"

phases:
  1:
    name: "Service Structure"
    hours: 4
    tasks:
      - "Create app/services/personality-ai/ directory structure"
      - "Create src/ subdirectory with modules"
      - "Set up __init__.py, config.py, service.py"
      - "Create requirements.txt with dependencies (langchain, sentence-transformers, faiss, etc.)"
  
  2:
    name: "LLM Integration"
    hours: 6
    tasks:
      - "Integrate with Tars-AI backend (192.168.1.26:8001) as primary LLM"
      - "Implement fallback to Ollama (localhost:11434) if Tars-AI unavailable"
      - "Create LLM client with retry logic and timeout handling"
      - "Add connection testing and health checks"
  
  3:
    name: "RAG Pipeline"
    hours: 8
    tasks:
      - "Build transcript ingestion from AudioTransfer voice training sessions"
      - "Implement vector embeddings using sentence-transformers"
      - "Set up vector database (FAISS or ChromaDB) for semantic search"
      - "Create semantic search for relevant context retrieval"
      - "Add document chunking and metadata management"
  
  4:
    name: "Conversation Analysis"
    hours: 4
    tasks:
      - "Implement tone analyzer (sentiment, formality, emotion detection)"
      - "Build style extractor (vocabulary patterns, sentence structure)"
      - "Create conversation pattern recognition"
      - "Build personality profile data structure with scoring"
  
  5:
    name: "Response Generator"
    hours: 4
    tasks:
      - "Implement prompt engineering for personality matching"
      - "Build response generator with RAG context injection"
      - "Add personality consistency scoring mechanism"
      - "Implement conversation turn management (>10 turns support)"
  
  6:
    name: "REST API"
    hours: 3
    tasks:
      - "Create app/backend/api/personality_ai.py FastAPI router"
      - "Implement /api/personality/analyze endpoint (conversation analysis)"
      - "Implement /api/personality/generate endpoint (response generation)"
      - "Implement /api/personality/profile endpoint (personality profile CRUD)"
      - "Add streaming response support (Server-Sent Events or WebSocket)"
  
  7:
    name: "Testing & Documentation"
    hours: 3
    tasks:
      - "Write unit tests for RAG pipeline components"
      - "Write integration tests for API endpoints"
      - "Document API in OpenAPI/Swagger spec"
      - "Create docs/services/personality-ai/README.md"
      - "Update CHANGELOG.md with [PERS] prefix entries"
